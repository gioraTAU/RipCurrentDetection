{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b15c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "# Google colab implemntation: https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/torchvision_finetuning_instance_segmentation.ipynb#scrollTo=6-NFR--2fXV3\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce04fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RipCurrentDataset(Dataset):\n",
    "    \"\"\" Rip current detector dataset. \"\"\"\n",
    "\n",
    "    def __init__(self, dframe, image_dir, transform=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param dframe: Dataframe object of csv file \"aug_data_label.csv\" generated by fix_size_and_aug\n",
    "        :param image_dir: path where all fixed size and augment images are saved\n",
    "        :param transform: the transform to be operated converting PIL image to torch tensor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = dframe\n",
    "        self.images_ids = self.df['Name'].unique()\n",
    "        data_dict = {}\n",
    "        \n",
    "        for d in dframe.to_dict(orient='records'):\n",
    "            data_dict[d['Name']] = {k: v if k=='Name' else int(v) for k, v in d.items()}\n",
    "        \n",
    "        self.data_dict = data_dict\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.images_ids.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        item : int\n",
    "            id number to get one image from the dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        img_tensor: torch.tensor\n",
    "            Image as torch tensor object ready to be inserted into the Deep Neural Network\n",
    "\n",
    "        target: dictionary\n",
    "            Python dictionary contains image id, bounding box location (x1, y1, x2, y2) and label 0 - no rip, 1 - rip\n",
    "\n",
    "        \"\"\"\n",
    "        img_name = self.images_ids[item]\n",
    "        img_data = self.data_dict[img_name]\n",
    "        \n",
    "        box = [img_data[key] for key in ['x1', 'y1', 'x2', 'y2']]\n",
    "        boxes = [box]\n",
    "        \n",
    "        area = abs(box[0] - box[2])*abs(box[1]-box[3])\n",
    "        areas = [area]\n",
    "        \n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        \n",
    "        label = torch.tensor(img_data['label'], dtype=torch.int64)\n",
    "\n",
    "        img = Image.open(self.image_dir + img_name).convert(\"RGB\")\n",
    "        img_tensor = self.transform(img) \n",
    "        \n",
    "        target = {\n",
    "            'image_id': torch.tensor([item]),\n",
    "            'boxes': boxes, \n",
    "#            0 labels for pics without the rip current\n",
    "            'labels': torch.tensor((img_data['label'],), dtype=torch.int64),\n",
    "            'iscrowd': torch.tensor((0,), dtype=torch.uint8),   \n",
    "            'area': areas\n",
    "            \n",
    "            \n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         x1, y1, x2, y2 = [torch.tensor(img_data[key], dtype=torch.int64) for key in ['x1', 'y1', 'x2', 'y2']]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         img_data = self.df[self.df['Name'] == img_name]\n",
    "\n",
    "\n",
    "\n",
    "#         x1, y1, x2, y2 = torch.tensor(img_data['x1'].values, dtype=torch.int64), torch.tensor(img_data['y1'].values, dtype=torch.int64), \\\n",
    "#                          torch.tensor(img_data['x2'].values, dtype=torch.int64), torch.tensor(img_data['y2'].values, dtype=torch.int64)\n",
    "#         label = torch.tensor(img_data['label'].values, dtype=torch.int64)\n",
    "\n",
    "\n",
    "#         target['image_id'] = torch.tensor(item)\n",
    "\n",
    "#         if label == 1:\n",
    "#             target['box'] = torch.cat((x1.unsqueeze(0), y1.unsqueeze(0), x2.unsqueeze(0), y2.unsqueeze(0)), dim=1)[0]\n",
    "#         else:\n",
    "#             target['box'] = torch.zeros((0, 4), dtype=torch.int64)\n",
    "\n",
    "#         target['labels'] = label[0]\n",
    "\n",
    "        return img_tensor, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968eb15",
   "metadata": {},
   "source": [
    "## 1 - Finetuning from a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819c83ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor,FasterRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "# load a model pre-trained on COCO\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "#This line is different from the toturial because of a deprecation warning\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (rip current) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e0e928",
   "metadata": {},
   "source": [
    "## 2 - Modifying the model to add a different backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48786c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# put the pieces together inside a FasterRCNN model\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659c2606",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b9eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transforms as T\n",
    "\n",
    "# def get_transform(train):\n",
    "# #     transforms = []\n",
    "#     transforms.append(T.ToTensor())\n",
    "#     if train:\n",
    "#         transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "#     return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4ccf60",
   "metadata": {},
   "source": [
    "### Testing forward() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8360b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from vision_utils import utils\n",
    "import torchvision.transforms as T\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "# data_loader = torch.utils.data.DataLoader(\n",
    "#  dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "#  collate_fn=utils.collate_fn)\n",
    "\n",
    "data_path = Path(r'..\\Data')\n",
    "df = pd.read_csv(data_path/'aug_data_labels.csv')\n",
    "df = df[df['label'] == 1]\n",
    "\n",
    "img_dir = str(data_path/'fixed_data') + '\\\\'\n",
    "trans = T.ToTensor()\n",
    "\n",
    "train_ds = RipCurrentDataset(df, img_dir, trans)\n",
    "data_loader = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=utils.collate_fn)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca80392",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For Training\n",
    "images,targets = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aa84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images = list(image for image in images)\n",
    "\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "output = model(images,targets)   # Returns losses and detections\n",
    "\n",
    "# For inference\n",
    "model.eval()\n",
    "# x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [images[1], images[2]]\n",
    "predictions= model(x)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a9b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912c2554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image, ImageDraw\n",
    "from matplotlib.pyplot import imshow\n",
    "import  matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def draw_rect(image, box):\n",
    "    \"\"\"Present image with bounding box on top\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : numpy.ndarray\n",
    "        numpy image\n",
    "\n",
    "    box: numpy.ndarray\n",
    "        Numpy array containing bounding boxes of shape `1 X 4` and the bounding boxes are represented in the\n",
    "        format `x1 y1 x2 y2`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    open new figure with image and bounding box\n",
    "\n",
    "    \"\"\"\n",
    "    box = box[0]\n",
    "    x1, y1, x2, y2 = box[0], box[1], box[2], box[3]\n",
    "    \n",
    "    img1 = ImageDraw.Draw(image)\n",
    "    shape = [(x1, y1), (x2, y2)]\n",
    "    img1.rectangle(shape, outline=\"red\", width=4)\n",
    "    \n",
    "    display(image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "idx = 1\n",
    "img = T.ToPILImage()(x[idx].squeeze_(0))\n",
    "\n",
    "# draw_rect(img, new_targets[0]['boxes'])\n",
    "for box, label in zip(predictions[idx]['boxes'], predictions[idx]['labels']):\n",
    "    if label == 1:\n",
    "        draw_rect(img, [box])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd19153f",
   "metadata": {},
   "source": [
    "##  Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709090d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RipCurrentDataset(df, img_dir, trans)\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=16, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db8f107",
   "metadata": {},
   "source": [
    "##  instantiate the model and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "# model = get_instance_segmentation_model(num_classes)\n",
    "\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4def118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision.models.detection.mask_rcnn\n",
    "sys.path.append(\"vision_utils\")\n",
    "import utils\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n",
    "    model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return metric_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8aca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    n_threads = torch.get_num_threads()\n",
    "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
    "    torch.set_num_threads(1)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = \"Test:\"\n",
    "\n",
    "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        model_time = time.time()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        evaluator_time = time.time()\n",
    "        coco_evaluator.update(res)\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ba881",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# let's train it for 10 epochs\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from engine import train_one_epoch, evaluate\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b137f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "x = [images[1], images[2]]\n",
    "predictions= model(x)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a3c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "img = T.ToPILImage()(x[idx].squeeze_(0))\n",
    "\n",
    "# draw_rect(img, new_targets[0]['boxes'])\n",
    "for box, label, score in zip(predictions[idx]['boxes'], predictions[idx]['labels'], predictions[idx]['scores']):\n",
    "    if label == 1 and score > 0.6:\n",
    "        draw_rect(img, [box])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
