{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80d33a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('../Data/aug_data_labels.csv'),\n",
       " WindowsPath('../Data/data_labels.csv'),\n",
       " WindowsPath('../Data/data_labels.txt'),\n",
       " WindowsPath('../Data/fixed_data'),\n",
       " WindowsPath('../Data/without_rips'),\n",
       " WindowsPath('../Data/without_rips-20220614T153319Z-001.zip'),\n",
       " WindowsPath('../Data/with_rips'),\n",
       " WindowsPath('../Data/with_rips-20220614T153300Z-001.zip'),\n",
       " WindowsPath('../Data/with_rips-20220614T153300Z-002.zip')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from costumDataset import *\n",
    "data_path = Path(r'..\\Data')\n",
    "list(data_path.glob('*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd1c3af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fbcc5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = data_path/'aug_data_labels.csv'\n",
    "csv_df = pd.read_csv(csv_path)\n",
    "csv_df.drop_duplicates(subset=['Name'], inplace=True)\n",
    "csv_df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56be0015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks to: https://stackoverflow.com/questions/72552647/pytorch-fasterrcnn-gives-no-output\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "import torchvision\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1813ae18",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The parameter '2' expected value 91 but got 2 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7d994beb95d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT,\n\u001b[0m\u001b[0;32m      9\u001b[0m                                                             \u001b[0mprogress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                                                              \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword_only_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py\u001b[0m in \u001b[0;36minner_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mweights_param\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefault_weights_arg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minner_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\detection\\faster_rcnn.py\u001b[0m in \u001b[0;36mfasterrcnn_resnet50_fpn\u001b[1;34m(weights, progress, num_classes, weights_backbone, trainable_backbone_layers, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[0mweights_backbone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m         \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ovewrite_value_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"categories\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m91\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py\u001b[0m in \u001b[0;36m_ovewrite_value_param\u001b[1;34m(param, new_value)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnew_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"The parameter '{param}' expected value {new_value} but got {param} instead.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The parameter '2' expected value 91 but got 2 instead."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from costumDataset import *\n",
    "data_path = Path(r'..\\Data')\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "import torchvision\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT,\n",
    "                                                            progress = True, \n",
    "                                                             num_classes = 2,\n",
    "                                                             trainable_backbone_layers =2,\n",
    "                                                           \n",
    "                                                            )\n",
    "df = pd.read_csv(data_path/'aug_data_labels.csv')\n",
    "df = df[df['label'] == 1]\n",
    "\n",
    "img_dir = str(data_path/'fixed_data') + '\\\\'\n",
    "trans = T.ToTensor()\n",
    "\n",
    "train_ds = RipCurrentDataset(df, img_dir, trans)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "for images, targets in train_dl:\n",
    "\n",
    "    \n",
    "    boxes = targets['box']\n",
    "    labels = targets['labels']\n",
    "    new_targets = []\n",
    "    for box, label in zip(boxes, labels):\n",
    "        tmp_d = {'boxes': box, 'labels': label}\n",
    "        new_targets.append(tmp_d)\n",
    "        assert tmp_d['boxes'].shape[0] == tmp_d['labels'].shape[0]\n",
    "        \n",
    "    assert len(images) == len(new_targets)\n",
    "    \n",
    "    output = model(images, new_targets)\n",
    "    model.eval()\n",
    "    \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9499848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83a6aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "new_targets[0]\n",
    "\n",
    "\n",
    "img = T.ToPILImage()(images[0].squeeze_(0))\n",
    "\n",
    "# draw_rect(img, new_targets[0]['boxes'])\n",
    "for box in predictions[0]['boxes']:\n",
    "    draw_rect(img, [box])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9173c0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[102.8281, 168.2507, 198.2979, 293.8721],\n",
       "          [ 36.2444,   3.3176,  72.7672,  73.2350],\n",
       "          [ 23.4281,   6.8971, 297.1317, 290.9916],\n",
       "          [ 15.3913,   0.0000, 292.0162, 289.3502],\n",
       "          [ 49.8298,  13.6729, 286.2994, 298.1857]], grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([16, 16, 23, 16, 17]),\n",
       "  'scores': tensor([0.2372, 0.2080, 0.0798, 0.0746, 0.0616], grad_fn=<IndexBackward0>)},\n",
       " {'boxes': tensor([[ 98.4122, 225.0630, 215.7074, 298.8628],\n",
       "          [ 69.7705, 207.5590,  94.2809, 223.1382],\n",
       "          [ 70.0334, 207.4812,  94.2688, 222.8797],\n",
       "          [117.6527,   6.9166, 283.9580, 132.7610],\n",
       "          [ 70.6363, 208.0485,  94.5545, 223.0778]], grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([64, 18, 16, 28, 21]),\n",
       "  'scores': tensor([0.3104, 0.2635, 0.1137, 0.0763, 0.0703], grad_fn=<IndexBackward0>)},\n",
       " {'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([], dtype=torch.int64),\n",
       "  'scores': tensor([], grad_fn=<IndexBackward0>)},\n",
       " {'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([], dtype=torch.int64),\n",
       "  'scores': tensor([], grad_fn=<IndexBackward0>)},\n",
       " {'boxes': tensor([[120.2237, 169.4512, 196.5509, 294.1746],\n",
       "          [  1.9380,   3.0375, 218.0477, 299.4455],\n",
       "          [116.9665, 171.3829, 194.8887, 295.0633]], grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([ 1,  1, 88]),\n",
       "  'scores': tensor([0.7853, 0.0676, 0.0517], grad_fn=<IndexBackward0>)},\n",
       " {'boxes': tensor([[164.2085, 146.0037, 244.0646, 294.7473],\n",
       "          [ 78.9831,  49.8257, 218.7459, 291.5859],\n",
       "          [ 97.5476,  99.9852, 235.3363, 293.3585],\n",
       "          [ 81.9132,  43.3796, 217.0379, 285.3318],\n",
       "          [168.5176, 173.9884, 198.8763, 193.0581],\n",
       "          [184.9600, 148.4208, 240.9651, 267.9228],\n",
       "          [ 93.2900,  34.9943, 246.8416, 288.8435],\n",
       "          [174.9961, 262.7225, 204.0425, 297.9234],\n",
       "          [ 94.8739,  60.2977, 180.6477, 255.1869]], grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([ 1, 22,  1, 19, 28, 64, 64, 31, 22]),\n",
       "  'scores': tensor([0.6342, 0.2126, 0.1782, 0.0929, 0.0708, 0.0673, 0.0659, 0.0649, 0.0509],\n",
       "         grad_fn=<IndexBackward0>)},\n",
       " {'boxes': tensor([[ 13.4475,  90.4553, 118.7821, 184.7301],\n",
       "          [ 15.6936,  87.7242, 124.9502, 180.1761]], grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([28, 42]),\n",
       "  'scores': tensor([0.6126, 0.0719], grad_fn=<IndexBackward0>)},\n",
       " {'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([], dtype=torch.int64),\n",
       "  'scores': tensor([], grad_fn=<IndexBackward0>)},\n",
       " {'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([], dtype=torch.int64),\n",
       "  'scores': tensor([], grad_fn=<IndexBackward0>)},\n",
       " {'boxes': tensor([[198.7311,  63.0147, 219.5454,  91.3350],\n",
       "          [193.9612,  75.1998, 218.1029,  91.5950],\n",
       "          [196.5042,  80.9640, 213.9306,  90.8444],\n",
       "          [202.4393,  85.1349, 218.0044,  90.9805],\n",
       "          [203.6997,  63.9811, 218.0135,  87.7283],\n",
       "          [187.5457,  63.5639, 228.4946,  92.7283]], grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([9, 9, 9, 9, 1, 9]),\n",
       "  'scores': tensor([0.9287, 0.2761, 0.2477, 0.1082, 0.0891, 0.0686],\n",
       "         grad_fn=<IndexBackward0>)},\n",
       " {'boxes': tensor([[101.2505,  19.7255, 295.1193, 287.6687]], grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([1]),\n",
       "  'scores': tensor([0.1476], grad_fn=<IndexBackward0>)},\n",
       " {'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([], dtype=torch.int64),\n",
       "  'scores': tensor([], grad_fn=<IndexBackward0>)},\n",
       " {'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([], dtype=torch.int64),\n",
       "  'scores': tensor([], grad_fn=<IndexBackward0>)},\n",
       " {'boxes': tensor([[100.3864,  88.8060, 172.1052, 106.2656],\n",
       "          [ 98.6319,  70.3164, 189.3013, 104.5878]], grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([42, 42]),\n",
       "  'scores': tensor([0.1962, 0.0722], grad_fn=<IndexBackward0>)},\n",
       " {'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([], dtype=torch.int64),\n",
       "  'scores': tensor([], grad_fn=<IndexBackward0>)},\n",
       " {'boxes': tensor([[ 89.6831,  15.6521, 270.7307, 297.2444]], grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([1]),\n",
       "  'scores': tensor([0.0530], grad_fn=<IndexBackward0>)}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ab61087",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "# For training\n",
    "\n",
    "\n",
    "images, boxes = torch.rand(4, 3, 600, 1200), torch.rand(4, 11, 4)\n",
    "boxes[:, :, 2:4] = boxes[:, :, 0:2] + boxes[:, :, 2:4]\n",
    "labels = torch.randint(1, 91, (4, 11))\n",
    "images = list(image for image in images)\n",
    "targets = []\n",
    "for i in range(len(images)):\n",
    "    d = {}\n",
    "    d['boxes'] = boxes[i]\n",
    "    d['labels'] = labels[i]\n",
    "    targets.append(d)\n",
    "    \n",
    "    \n",
    "output = model(images, targets)\n",
    "\n",
    "\n",
    "# For inference\n",
    "model.eval()\n",
    "\n",
    "\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "\n",
    "predictions = model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eaea63f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[0.0363, 0.7126, 0.5770, 0.9409],\n",
       "         [0.7278, 0.8902, 1.0662, 1.0080],\n",
       "         [0.0739, 0.9098, 1.0553, 1.1821],\n",
       "         [0.0848, 0.1756, 1.0565, 0.9715],\n",
       "         [0.4173, 0.8681, 0.8439, 1.0397],\n",
       "         [0.8167, 0.1354, 1.6879, 0.8188],\n",
       "         [0.8472, 0.3822, 1.6970, 1.2252],\n",
       "         [0.8038, 0.3779, 0.9716, 0.6015],\n",
       "         [0.5040, 0.4569, 1.3133, 1.1090],\n",
       "         [0.7390, 0.3664, 1.1639, 1.3458],\n",
       "         [0.4682, 0.4418, 0.8152, 0.6511]]),\n",
       " 'labels': tensor([72,  6,  1, 90,  1, 71, 27, 73, 78,  6, 59])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c51b585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00accf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path/'aug_data_labels.csv')\n",
    "df = df[df['label'] == 1]\n",
    "\n",
    "img_dir = str(data_path/'fixed_data') + '\\\\'\n",
    "trans = T.ToTensor()\n",
    "\n",
    "train_ds = RipCurrentDataset(df, img_dir, trans)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "\n",
    "a = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c795fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from costumDataset import *\n",
    "from resize_and_augmentation import *\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(r'..\\Data')\n",
    "S = np.array((300, 300))\n",
    "data_labels_path = str(data_path/'data_labels.csv')\n",
    "im_path = str(data_path/'with_rips') + '\\\\'\n",
    "target_path = str(data_path/'fixed_data') + '\\\\'\n",
    "fix_size_and_aug(data_labels_path, S, im_path, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631e5d43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
